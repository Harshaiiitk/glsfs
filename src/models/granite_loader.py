# src/models/granite_loader.py
"""
Granite Model Loader for GLSFS - BALANCED: Fast AND Accurate

OPTIMIZATIONS THAT DON'T HURT ACCURACY:
=======================================
âœ… Use MPS (Apple Silicon GPU) - 5-10x faster
âœ… Use float16 - 2x faster
âœ… Use inference_mode - faster
âœ… Use KV-cache - faster
âœ… Warmup - faster subsequent calls

SETTINGS RESTORED FOR ACCURACY:
===============================
âœ… max_new_tokens=200 (enough for complex commands)
âœ… do_sample=True with temperature=0.1 (slight creativity, avoids loops)
âœ… Full system prompt (better context)

EXPECTED PERFORMANCE:
====================
- Speed: 3-8 seconds (same as before)
- Accuracy: Same as original model âœ“
"""

import torch
import os
import re
import time


class GraniteCommandGenerator:
    def __init__(self, model_path=None):
        """
        Initialize with your fine-tuned Granite model.
        Balanced for speed AND accuracy.
        """
        if model_path is None:
            model_path = os.path.expanduser("~/glsfs/src/models/granite/glsfs_granite_finetuned")
        
        model_path = os.path.expanduser(model_path)
        model_path = os.path.abspath(model_path)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ Model not found at {model_path}")
        
        print(f"â³ Loading model from {model_path}...")
        
        # Determine best device
        self.device = self._get_best_device()
        print(f"ðŸ–¥ï¸  Using device: {self.device}")
        
        # Determine best dtype
        self.dtype = self._get_best_dtype()
        print(f"ðŸ“Š Using dtype: {self.dtype}")
        
        # Load model
        self._load_model(model_path)
        
        print(f"âœ… Model ready for inference!\n")
    
    def _get_best_device(self):
        """Get the fastest available device."""
        if torch.backends.mps.is_available():
            print("   âœ… Apple Silicon detected - using MPS acceleration")
            return "mps"
        
        if torch.cuda.is_available():
            print(f"   âœ… CUDA detected - using GPU: {torch.cuda.get_device_name(0)}")
            return "cuda"
        
        print("   âš ï¸  No GPU detected - using CPU (slower)")
        return "cpu"
    
    def _get_best_dtype(self):
        """Get the best dtype for speed/memory."""
        if self.device in ["mps", "cuda"]:
            return torch.float16
        return torch.float32
    
    def _load_model(self, model_path):
        """Load model with speed optimizations."""
        
        # Try Unsloth first
        try:
            from unsloth import FastLanguageModel
            
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path,
                max_seq_length=2048,
                dtype=None,
                load_in_4bit=True,
            )
            
            FastLanguageModel.for_inference(self.model)
            self.using_unsloth = True
            print("   âœ… Loaded with Unsloth (4-bit quantized)")
            return
            
        except ImportError:
            print("   âš ï¸  Unsloth not available, using transformers...")
            self.using_unsloth = False
        except Exception as e:
            print(f"   âš ï¸  Unsloth failed: {e}, trying transformers...")
            self.using_unsloth = False
        
        # Fallback to transformers
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print("   â³ Loading tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("   â³ Loading model...")
        load_start = time.time()
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=self.dtype,
            low_cpu_mem_usage=True,
        )
        
        print(f"   â³ Moving model to {self.device}...")
        self.model = self.model.to(self.device)
        self.model.eval()
        
        load_time = time.time() - load_start
        print(f"   âœ… Model loaded in {load_time:.1f}s")
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def generate_command(self, user_query):
        """
        Generate bash command from natural language query.
        Balanced for speed AND accuracy.
        """
        start_time = time.time()
        
        # FULL system prompt for accuracy (same as training)
        system_message = (
            "You are an expert Linux filesystem assistant. "
            "When users ask about file operations, provide accurate bash commands with clear explanations. "
            "For dangerous operations, include warnings."
        )
        
        formatted_prompt = (
            f"System: {system_message}\n\n"
            f"User: {user_query}\n\n"
            f"Assistant:"
        )
        
        # Tokenize
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048,  # Full context window
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Generate with BALANCED settings
        with torch.no_grad():
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    # ACCURACY SETTINGS (restored)
                    max_new_tokens=200,       # Enough for complex commands
                    do_sample=True,           # Slight randomness avoids loops
                    temperature=0.1,          # Very low = mostly deterministic but not stuck
                    top_p=0.9,                # Nucleus sampling for quality
                    
                    # SPEED SETTINGS (kept)
                    num_beams=1,              # No beam search
                    use_cache=True,           # KV-cache for speed
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
        
        # Decode
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract response
        if "Assistant:" in generated_text:
            response_text = generated_text.split("Assistant:")[-1].strip()
        else:
            response_text = generated_text.strip()
        
        # Parse command and explanation
        command, explanation = self._parse_response(response_text)
        
        # Normalize paths
        command = self._normalize_paths(command)
        
        inference_time = time.time() - start_time
        print(f"   âš¡ Inference time: {inference_time:.2f}s")
        
        return {
            'command': command,
            'explanation': explanation,
            'raw_response': response_text,
            'inference_time': inference_time
        }
    
    def _normalize_paths(self, command):
        """
        Normalize paths to Docker paths.
        Handles: ~, $HOME, case sensitivity, relative paths.
        """
        if not command:
            return command
        
        # Fix common model output bugs
        command = command.replace('find.', 'find .')
        command = command.replace('ls.', 'ls .')
        
        # Expand ~ and $HOME
        command = command.replace('~/', '/home/user/')
        command = re.sub(r'~(?=\s|$)', '/home/user', command)
        command = command.replace('$HOME/', '/home/user/')
        command = command.replace('${HOME}/', '/home/user/')
        command = re.sub(r'\$HOME(?=\s|$)', '/home/user', command)
        command = re.sub(r'\$\{HOME\}(?=\s|$)', '/home/user', command)
        
        # Fix case sensitivity for folder names
        # desktop -> Desktop, documents -> Documents, etc.
        folder_fixes = [
            ('desktop', 'Desktop'),
            ('documents', 'Documents'),
            ('downloads', 'Downloads'),
        ]
        
        for wrong_case, correct_case in folder_fixes:
            # Fix in /home/user/ paths
            pattern = rf'/home/user/{wrong_case}(?=/|$|\s)'
            command = re.sub(pattern, f'/home/user/{correct_case}', command, flags=re.IGNORECASE)
            
            # Fix bare folder names (but not in quoted patterns)
            # This handles: ls documents, find Documents/, cat desktop/file.txt
            # Split and fix tokens that aren't in quotes
            command = self._fix_folder_case_in_command(command, wrong_case, correct_case)
        
        return command
    
    def _fix_folder_case_in_command(self, command, wrong_case, correct_case):
        """Fix folder case in command, avoiding quoted strings."""
        # Simple approach: fix obvious patterns
        # Pattern: word boundary + folder name + (/ or end or space)
        
        # Don't fix if already in /home/user/ path (handled above)
        if f'/home/user/{correct_case}' in command:
            return command
        
        # Fix bare folder reference at word boundary
        # (?<![/\w]) = not preceded by / or word char
        # (?=/|\s|$) = followed by /, space, or end
        pattern = rf'(?<![/\w])({wrong_case})(?=/|\s|$)'
        
        def replacer(m):
            return correct_case
        
        command = re.sub(pattern, replacer, command, flags=re.IGNORECASE)
        
        # Also fix folder/subpath patterns
        pattern = rf'(?<![/\w])({wrong_case})/'
        command = re.sub(pattern, f'{correct_case}/', command, flags=re.IGNORECASE)
        
        return command
    
    def _parse_response(self, response_text):
        """Parse response to extract command and explanation."""
        lines = response_text.strip().split('\n')
        
        command_lines = []
        explanation_lines = []
        found_explanation = False
        
        for line in lines:
            stripped = line.strip()
            
            if not found_explanation:
                # Check if this line looks like a command
                if stripped and not stripped.startswith('#') and not stripped.startswith('This') and not stripped.startswith('The '):
                    # Check if it's a bash command
                    if self._looks_like_command(stripped):
                        command_lines.append(stripped)
                    else:
                        found_explanation = True
                        explanation_lines.append(stripped)
                elif stripped.startswith('#') or stripped.startswith('This') or stripped.startswith('The '):
                    found_explanation = True
                    explanation_lines.append(stripped)
                elif stripped == '':
                    if command_lines:
                        found_explanation = True
            else:
                explanation_lines.append(stripped)
        
        command = ' '.join(command_lines).strip()
        explanation = ' '.join(explanation_lines).strip()
        
        # Fallback if no command found
        if not command and lines:
            # Try to extract command from first non-empty line
            for line in lines:
                stripped = line.strip()
                if stripped and self._looks_like_command(stripped):
                    command = stripped
                    break
            
            # Last resort: use first line
            if not command:
                command = lines[0].strip()
        
        # Clean command
        command = self._clean_command(command)
        
        return command, explanation
    
    def _looks_like_command(self, text):
        """Check if text looks like a bash command."""
        command_starters = [
            'find', 'ls', 'grep', 'du', 'df', 'cat', 'head', 'tail',
            'sort', 'chmod', 'rm', 'cp', 'mv', 'mkdir', 'touch', 'echo',
            'wc', 'file', 'stat', 'tree', 'pwd', 'less', 'more',
            'awk', 'sed', 'cut', 'uniq', 'diff', 'tar', 'zip', 'unzip'
        ]
        
        first_word = text.split()[0].lower() if text.split() else ''
        
        # Remove any leading $ or #
        first_word = first_word.lstrip('$#')
        
        return first_word in command_starters
    
    def _clean_command(self, command):
        """Clean up command formatting."""
        if not command:
            return command
        
        # Remove shell prompt prefixes
        for prefix in ['$ ', '# ', '> ']:
            if command.startswith(prefix):
                command = command[len(prefix):]
        
        # Remove markdown code formatting
        if command.startswith('```bash'):
            command = command[7:]
        if command.startswith('```'):
            command = command[3:]
        if command.endswith('```'):
            command = command[:-3]
        if command.startswith('`') and command.endswith('`'):
            command = command[1:-1]
        
        # Fix spacing issues
        command = command.replace('find.', 'find .')
        command = command.replace('ls.', 'ls .')
        
        return command.strip()
    
    def warmup(self):
        """Run a warmup inference for faster subsequent calls."""
        print("ðŸ”¥ Warming up model...")
        start = time.time()
        _ = self.generate_command("list files")
        warmup_time = time.time() - start
        print(f"âœ… Warmup complete ({warmup_time:.2f}s)\n")